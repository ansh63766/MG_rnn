{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXgMJRLR0Zo1"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf #we are using tf 2 here \n",
        "\n",
        "!pip install mitdeeplearning\n",
        "import mitdeeplearning as mdl\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm import tqdm\n",
        "!apt-get install abcmidi timidity > /dev/null 2>&1\n",
        "\n",
        "assert len(tf.config.list_physical_devices('GPU')) > 0\n",
        "\n",
        "\n",
        "\n",
        "songs = mdl.lab1.load_training_data()\n",
        "\n",
        "example_song = songs[0]\n",
        "print(\"\\nExample song: \")\n",
        "print(example_song)\n",
        "\n",
        "mdl.lab1.play_song(example_song)\n",
        "\n",
        "songs_joined = \"\\n\\n\".join(songs) \n",
        "\n",
        "vocab = sorted(set(songs_joined))\n",
        "print(\"There are\", len(vocab), \"unique characters in the dataset\")\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')\n",
        "\n",
        "def vectorize_string(string):\n",
        "    vectorized = np.array([char2idx[char] for char in string])\n",
        "    return vectorized\n",
        "\n",
        "vectorized_songs = vectorize_string(songs_joined)\n",
        "\n",
        "print ('{} ---- characters mapped to int ----> {}'.format(repr(songs_joined[:10]), vectorized_songs[:10]))\n",
        "assert isinstance(vectorized_songs, np.ndarray), \"returned result should be a numpy array\"\n",
        "\n",
        "\n",
        "def get_batch(vectorized_songs, seq_length, batch_size):\n",
        "    n = vectorized_songs.shape[0] - 1\n",
        "    idx = np.random.choice(n - seq_length, batch_size)\n",
        "\n",
        "    input_batch = [vectorized_songs[i:i + seq_length] for i in idx]\n",
        "    output_batch = [vectorized_songs[i + 1:i + seq_length + 1] for i in idx]\n",
        "\n",
        "    x_batch = np.reshape(input_batch, [batch_size, seq_length])\n",
        "    y_batch = np.reshape(output_batch, [batch_size, seq_length])\n",
        "    return x_batch, y_batch\n",
        "\n",
        "\n",
        "test_args = (vectorized_songs, 10, 2)\n",
        "if not mdl.lab1.test_batch_func_types(get_batch, test_args) or \\\n",
        "   not mdl.lab1.test_batch_func_shapes(get_batch, test_args) or \\\n",
        "   not mdl.lab1.test_batch_func_next_step(get_batch, test_args): \n",
        "   print(\"======\\n[FAIL] could not pass tests\")\n",
        "else: \n",
        "   print(\"======\\n[PASS] passed all tests!\")\n",
        "\n",
        "\n",
        "x_batch, y_batch = get_batch(vectorized_songs, seq_length=5, batch_size=1)\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(np.squeeze(x_batch), np.squeeze(y_batch))):\n",
        "    print(\"Step {:3d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n",
        "\n",
        "def LSTM(rnn_units): \n",
        "  return tf.keras.layers.LSTM(\n",
        "    rnn_units, \n",
        "    return_sequences=True, \n",
        "    recurrent_initializer='glorot_uniform',\n",
        "    recurrent_activation='sigmoid',\n",
        "    stateful=True,\n",
        "  )\n",
        "\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "\n",
        "        LSTM(rnn_units),\n",
        "\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model(len(vocab), embedding_dim=256, rnn_units=1024, batch_size=32)\n",
        "model.summary()\n",
        "x, y = get_batch(vectorized_songs, seq_length=100, batch_size=32)\n",
        "pred = model(x)\n",
        "print(\"Input shape:      \", x.shape, \" # (batch_size, sequence_length)\")\n",
        "print(\"Prediction shape: \", pred.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "sampled_indices = tf.random.categorical(pred[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "sampled_indices\n",
        "\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[x[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))\n",
        "\n",
        "def compute_loss(labels, logits):\n",
        "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "    return loss\n",
        "\n",
        "example_batch_loss = compute_loss(y, pred)\n",
        "\n",
        "print(\"Prediction shape: \", pred.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "num_training_iterations = 2000  \n",
        "batch_size = 4  \n",
        "seq_length = 100  \n",
        "learning_rate = 5e-3  \n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256 \n",
        "rnn_units = 1024  \n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"my_ckpt\")\n",
        "\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y): \n",
        "    with tf.GradientTape() as tape:\n",
        "        y_hat = model(x)\n",
        "\n",
        "        loss = compute_loss(y, y_hat)\n",
        "\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "history = []\n",
        "plotter = mdl.util.PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "if hasattr(tqdm, '_instances'):\n",
        "    tqdm._instances.clear()  \n",
        "\n",
        "for iter in tqdm(range(num_training_iterations)):\n",
        "    x_batch, y_batch = get_batch(vectorized_songs, seq_length, batch_size)\n",
        "    loss = train_step(x_batch, y_batch)\n",
        "\n",
        "    history.append(loss.numpy().mean())\n",
        "    plotter.plot(history)\n",
        "\n",
        "    if iter % 100 == 0:\n",
        "        model.save_weights(checkpoint_prefix)\n",
        "\n",
        "model.save_weights(checkpoint_prefix)\n",
        "\n",
        "model = build_model(len(vocab), embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "def generate_text(model, start_string, generation_length=1000):\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    model.reset_states()\n",
        "\n",
        "    for i in range(generation_length):\n",
        "        predictions = model(input_eval)\n",
        "\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "\n",
        "generated_text = generate_text(model, start_string=\"X\", generation_length=1000)\n",
        "\n",
        "\n",
        "\n",
        "generated_songs = mdl.lab1.extract_song_snippet(generated_text)\n",
        "\n",
        "for i, song in enumerate(generated_songs): \n",
        "  waveform = mdl.lab1.play_song(song)\n",
        "\n",
        "  if waveform:\n",
        "    print(\"Generated song\", i)\n",
        "    ipythondisplay.display(waveform)"
      ]
    }
  ]
}